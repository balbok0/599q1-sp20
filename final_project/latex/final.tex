\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsfonts}
\usepackage{braket}
\usepackage{graphicx}
% \usepackage{thmbox}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage[dvipsnames]{xcolor}
\usepackage{bm}
\usepackage{indentfirst}
\usepackage{qcircuit}

\usepackage[colorlinks,allcolors=blue]{hyperref} % optional
\usepackage[noabbrev,capitalize,nameinlink]{cleveref}

\usepackage[ruled,vlined]{algorithm2e}

\usepackage[style=numeric]{biblatex}
\addbibresource{references.bib}

\usepackage{geometry}
\geometry{margin=1in}

\title{Quantum Gradient Descent via Jordan's Method}
\author{Jakub Filipek}
\date{June 2020}



\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\identity}{\mathds{1}}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{claim}{Claim}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{fact}{Fact}
\newtheorem{corr}{Corollary}[section]
\newtheorem{define}{Definition}
\newtheorem{problem}{Problem}



\begin{document}

\maketitle

\begin{abstract}
    \noindent Gradient Descent has been a key technique for optimizing many Machine Learning Algorithms.
    However, due to extremely large number of parameters of modern models (up to $10^{11}$ floating points)
    this problem has not been widely discussed in Quantum Computing Community.
    In this project, I will try to implement algorithm presented in~\cite{Jordan_2005}.
    Additionally I will follow by discussing potential improvements of this paper, as discussed in~\cite{Gily_n_2019}.
    Both of these are done on example of $f(x) = x^2$, with domain being $[0, 2^{\vec{n}}]$.
\end{abstract}

% \section{Introduction}

\section{Introduction to Gradient Descent}
\label{sec:intro-grad-desc}

\subsection{Gradient Descent}
\label{subsec:grad-desc}
Gradient Descent is trying to solve a problem of minimizing (or maximizing) a function:
\begin{align}
    \arg \min \limits_x f(x) | x \in X
\end{align}, where $X$ is a continuos space of numbers (typically in $\mathcal{R}^d$).

Since often finding a global minimum is computationally impossible, a lot of problems reduce to finding local minimum from a given starting point,
and then repeating an experiment for set number of random starting points. In particular, vanilla gradient descent algorithm looks as follows:
\begin{algorithm}[h]
    \SetAlgoLined
    \KwResult{An approximation of local minima of $f$ for starting point $\pmb{w}$, and dataset $X$}
    Let $f$ be parametrized by $\pmb{w}$. \;
    Let $\pmb{w}_0 = \pmb{w}$ \;
    \For{i = 0 to T}{
        Let $\nabla_{\pmb{w}} f(X)$ be an average of gradients of $f$ over dataset X, all with respect to $\pmb{w}$ \;
        $\pmb{w}_{i + 1} = \pmb{w}_i - \alpha \nabla_{\pmb{w}} f(X)$\;
    }
    \Return{$\pmb{w}_T$}
    \caption{Gradient Descent Algorithm}
    \label{alg:vanilla-grad-desc}
\end{algorithm}

While this algorithm does not necessarily lead to the local minimum it performs exceptionally well in practice. It also is a basis for a family of
\textit{gradient descent} algorithms which have been a backbone of last decade's improvements in Machine Learning.

More importantly however, by the above algorithm we can see that we need to calculate the above gradient $O(Td)$ times for each initialization point.
Additionally if we want to achieve $\epsilon$-precise result we need to have $|X| \in O(\frac{1}{\epsilon^2})$, if sampled randomly.

Hence overall, this naive algorithm will take:
\begin{align}
    O(\frac{dTN}{\epsilon^2})
\end{align}
gradient calculations, where $N$ is number of random starting points of $\pmb{w}$.

\subsection{Important Technicalities of Gradient Calculation}
\label{subsec:tech-grad}

Classically, gradient at point $\pmb{w}$ is calculated using standard derivative calculation for each dimension:
\begin{align}
    \label{eq:grad1}
    \nabla_{w_i} &= \frac{f(\pmb{w} + \pmb{h_i}) - f(\pmb{w})}{h} \\
    \pmb{h_i} &= \begin{pmatrix}
        0, 0, \hdots, h, \hdots, 0
    \end{pmatrix} & \text{where $h$ is at index $i$}
\end{align}

As we can see the $f(\pmb{w})$ can be reused across all dimensions, and hence we require $f$ to be called $d + 1$ times classically.

Alternatively, we can switch Equation~\ref{eq:grad1} to:
\begin{align}
    \label{eq:grad2}
    \nabla_{w_i} &= \frac{f(\pmb{w} + \frac{\pmb{h_i}}{2}) - f(\pmb{w} - \frac{\pmb{h_i}}{2})}{h}
\end{align}
which the same definition of $\pmb{h_i}$ as above. This requires $f$ to be called $2d$ times, since nothing can be reused.

The Jordan algorithm calculates gradient using the first method, but has a few inspirations from second method.

Additionally for future use, we can rewrite Equation~\ref{eq:grad1}:
\begin{align}
    \label{eq:grad-lemma}
    f(\pmb{w} + \pmb{h_i}) &= f(\pmb{w}) + h\nabla_{w_i}
\end{align}

\section{Jordan's Algorithm}
\label{sec:jord-alg}
\subsection{Prerequisites}
\label{sec:jord-preq}
In this section I will try to provide a much more explicit explanation of needed function oracle,
and its interpretation than presented in the original paper.

Firstly, let us define a classical function $f: x \mapsto y$, where both the domain and the range are fixed point numbers (those can be though of as integers divided by some $2^p$ number).
Let input be $n$-bit precise, and the output to be $2^{n_o}$-bit precise. This means that, for example, if the domain and range are both $[0, 1]$, the distances between distinct inputs and outputs have to be $2^{-n}$ and $2^{-n_o}$, respectively.

For ease of future notation let $2^n = N$ and $2^{n_o} = N_o$.

This allows us to create $f'$ such that $f': x' \mapsto y'$, where $x' \in [0, N]$ and $y' \in [0, N_o]$.
$f'$ on the other hand can be easily converted into a quantum oracle such that:
\begin{align}
    O_f: \ket{x}\ket{a} \mapsto \ket{x}\ket{(a + f'(x)) \mod N_o}
\end{align}

Let us combine this thinking with the gradient calculation from Equation~\ref{eq:grad2}. Let $x \in [-\frac{l}{2}, \frac{l}{2}]$.
Then conversion $f \rightarrow f'$ would split $l$ into $N$ integers, and shift $x$ by $\frac{l}{2}$.

Hence corresponding $f(x) = \frac{l}{N}f'(x' + \frac{N}{2})$, or going the other way:
\begin{equation}
    \label{eq:func-conversion}
    f'(x') = \frac{N}{l}f(x - \frac{l}{2}) = \frac{N}{l}f(\frac{l}{N}(x' - \frac{N}{2}))
\end{equation}

And similarly for the $O_f$ corresponding to the $f'$.

This particular relation between $f, f'$ and $O_f$ is not explicit in~\cite{Jordan_2005},
and only can be understood by transformations between equations, while the transformation from $f$ to $O_f$ is described as trivial.
I believe that because of that this paper lacks a lot of clarity and makes it much harder to reproduce experimentally.

\subsection{Algorithm Explanation}
\label{sec:jord-expl}
Jordan Algorithm is still based on a vanilla gradient descent as mentioned in Section~\ref{subsec:grad-desc},
however gradient calculation itself it done on quantum device as described in~\cite{Jordan_2005}, which is as follows.

An important note is that this gradient is calculated for $\pmb{w} = \pmb{0}$, but we can just shift any function to center it at $0$ perform this calculation, and then shift it back.

\begin{figure}[h]
    \[\Qcircuit @C=1em @R=1em {
        &                 & \gate{H}                    & \qw                        & \multigate{7}{O_f} & \multigate{1}{QFT}         & \qw \\
        &                 & \gate{H}                    & \qw                        & \ghost{O_f}        & \ghost{QFT}                & \qw
        \inputgroupv{1}{2}{0.7em}{1.1em}{\ket{w_1}} \\
        & \lstick{\vdots} & \gate{H^{\otimes (d - 2)n}} & \qw                        & \ghost{O_f}        & \gate{QFT^{\otimes d - 2}} & \qw \\
        &                 & \gate{H}                    & \qw                        & \ghost{O_f}        & \multigate{1}{QFT}         & \qw \\
        &                 & \gate{H}                    & \qw                        & \ghost{O_f}        & \ghost{QFT}                & \qw
        \inputgroupv{4}{5}{0.7em}{1.1em}{\ket{w_{d}}} \\
        &                 & \qw                         & \multigate{2}{QFT^\dagger} & \ghost{O_f}        & \qw                        & \qw \\
        &                 & \qw                         & \ghost{QFT^\dagger}        & \ghost{O_f}        & \qw                        & \qw \\
        &                 & \gate{X}                    & \ghost{QFT^\dagger}        & \ghost{O_f}        & \qw                        & \qw
        \inputgroupv{6}{8}{0.7em}{2em}{\ket{\pmb{out}}} \\
    }\]
    \label{qc:jordans-alg}
    \caption{Circuit Visualization for Jordan's Algorithm}
\end{figure}

Then the above circuit (where inputs are all $\ket{0}$'s) will generate $\ket{\frac{N}{m}\frac{\partial f}{\partial w_1}}, \ket{\frac{N}{m}\frac{\partial f}{\partial w_2}}, \hdots$.

To show this let us consider the transformations of this circuit:
\begin{align}
    \ket{0}^{\otimes nd}\ket{0}^{n_o} &\xrightarrow{H's, X} \\
    \frac{1}{\sqrt{N^d}} \sum\limits_{w} \ket{\pmb{w}} \ket{\pmb{1}} &\xrightarrow{QFT^\dagger} \\
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{w}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{a} &\xrightarrow{O_f} \\
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{w}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{(a + f'(\pmb{w})) \mod 2^N_o} &= \\
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{w}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a + f'(\pmb{w})}{N_o}}\ket{a} &= \\
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{w}e^{i2\pi\frac{f'(\pmb{w})}{N_o}}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{a} &= \\
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{w}e^{i2\pi\frac{N}{ml}f(\frac{l}{N}(w - \frac{N}{2}))}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{a} & & \text{from Equation~\ref{eq:func-conversion}}
\end{align}

Note the appearance of $m$ in the last line. I can be thought of conversion of $N_o$ (max magnitude of $f'$) into magnitude of $f$, and is a maximal magnitude of $\nabla_f$ over the domain given domain.
While I show derivation of relation between $n_o$ and $m$, I will later explicitly use the formula derived in~\cite{Jordan_2005}.

Continuing analysis of the circuit:
\begin{align}
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{w}e^{i2\pi\frac{N}{ml}f(\frac{l}{N}(w - \frac{N}{2}))}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{a} &= \\
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{w}e^{i2\pi\frac{N}{ml}(f(0) + \frac{l}{N}(w - \frac{N}{2})\nabla_f)}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{a} &= & \text{from Equation~\ref{eq:grad-lemma}} \\
    \frac{e^{i2\pi\frac{N}{ml}(f(0) - \frac{N}{2m})}}{\sqrt{N^d N_o}} \sum\limits_{w}e^{i2\pi\frac{w}{m}\nabla_f}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{a} &= \\
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{w}e^{i2\pi\frac{w}{m}\nabla_f}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{a} &\xrightarrow{QFT} & \text{Ignoring Global Phase}\\
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{i = 1}^d \ket{\frac{N}{m}\frac{\partial f}{\partial w_i}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{a} & & \text{Since $\nabla_f$ is just a vector of partial derivatives}
\end{align}

And hence we get a scaled gradient in the input registers.

\section{Numerical Experiment}
\subsection{Problem Description}
\subsection{Implementation}
\subsection{Results}

\section{Improvements to Jordan's Algorithm}
\subsection{Different Way of Expressing Jordan's Algorithm}
\subsection{Amplitude Amplification}
\subsection{Grover Search}
\subsection{Additional Improvements}
\textbf{\textcolor{red}{Fix section title}}

\section{Devil's Advocate: Improvements to Classical Gradient Calculation}
\subsection{Active Sampling vs. Passive Sampling}

\section{Conclusion}
% \newpage
\printbibliography
\end{document}



% This is actually the Taylor series for $e^x$; so, substituting, we see:

% \begin{align*}
%     &= 2 \Big[ \sum_{k = 0}^{\infty} \frac{1}{k!} \Big(\frac{2\lambda t}{N}\Big)^k - 1 - \Big(\frac{2\lambda t}{N}\Big) \Big]
% \end{align*}