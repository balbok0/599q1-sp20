\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsfonts}
\usepackage{braket}
\usepackage{graphicx}
% \usepackage{thmbox}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage[dvipsnames]{xcolor}
\usepackage{bm}
\usepackage{indentfirst}
\usepackage{qcircuit}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    commentstyle=\color{codegreen},
    keywordstyle=\color{codegreen},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}


\usepackage[colorlinks,allcolors=blue]{hyperref} % optional
\usepackage[noabbrev,capitalize,nameinlink]{cleveref}

\usepackage[ruled,vlined]{algorithm2e}

\usepackage[style=numeric]{biblatex}
\addbibresource{references.bib}

\usepackage{geometry}
\geometry{margin=1in}

\title{Quantum Gradient Descent via Jordan's Method}
\author{Jakub Filipek}
\date{June 2020}



\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\identity}{\mathds{1}}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{claim}{Claim}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{fact}{Fact}
\newtheorem{corr}{Corollary}[section]
\newtheorem{define}{Definition}
\newtheorem{problem}{Problem}


\setcounter{section}{-1}
\begin{document}

\maketitle

\begin{abstract}
    \noindent Gradient Descent has been a key technique for optimizing many Machine Learning Algorithms.
    However, due to extremely large number of parameters of modern models (up to $10^{11}$ floating points)
    this problem has not been widely discussed in Quantum Computing Community.
    In this project, I will try (and fail) implementing algorithm presented in~\cite{Jordan_2005}.
    Additionally I will follow by discussing potential improvements of that paper, as discussed in~\cite{Gily_n_2019}.
    Both of these are done on example of $f(x) = x^2$.
\end{abstract}

\section{Project Report Organization}
I will start with the introduction of classical gradient descent algorithm, which will be unchanged,
and classical gradient calculation. In Section~\ref{sec:jord-alg} I will explain Jordan's Algorithm,
provide an explicit circuit, and do a much more explicit analysis of it than done originally in~\cite{Jordan_2005}.

In Section~\ref{sec:num-exp} I will describe the experimental procedure, show results, as well as discuss possible reasons
for the failure of this experiment.

Section~\ref{sec:jor-improv} will discuss improvements to Jordan's Algorithm presented in~\cite{Gily_n_2019} on a more abstract level.
This is followed by similarly abstract description of improvements done in classical gradient calculation in Section~\ref{sec:class-imp}.

The project report ends with conclusion in Section~\ref{sec:conclusion}.

\section{Introduction to Gradient Descent}
\label{sec:intro-grad-desc}

\subsection{Gradient Descent}
\label{subsec:grad-desc}
Gradient Descent is trying to solve a problem of minimizing (or maximizing) a function:
\begin{align}
    \arg \min \limits_x f(x) | x \in X
\end{align}, where $X$ is a continuos space of numbers (typically in $\mathcal{R}^d$).

Since often finding a global minimum is computationally impossible, a lot of problems reduce to finding local minimum from a given starting point,
and then repeating an experiment for set number of random starting points. In particular, vanilla gradient descent algorithm looks as follows:

\begin{algorithm}[H]
    \SetAlgoLined
    \KwResult{An approximation of local minima of $f$ for starting point $\pmb{w}$, and dataset $X$}
    Let $f$ be parametrized by $\pmb{w}$. \;
    Let $\pmb{w}_0 = \pmb{w}$ \;
    \For{i = 0 to T}{
        Let $\nabla_{\pmb{w}} f(X)$ be an average of gradients of $f$ over dataset X, all with respect to $\pmb{w}$ \;
        $\pmb{w}_{i + 1} = \pmb{w}_i - \alpha \nabla_{\pmb{w}} f(X)$\;
    }
    \Return{$\pmb{w}_T$}
    \caption{Gradient Descent Algorithm}
    \label{alg:vanilla-grad-desc}
\end{algorithm}

While this algorithm does not necessarily lead to the local minimum it performs exceptionally well in practice. It also is a basis for a family of
\textit{gradient descent} algorithms which have been a backbone of last decade's improvements in Machine Learning.

More importantly however, by the above algorithm we can see that we need to calculate the above gradient $O(Td)$ times for each initialization point.
Additionally if we want to achieve $\epsilon$-precise result we need to have $|X| \in O(\frac{1}{\epsilon^2})$, if sampled randomly.

Hence overall, this naive algorithm will take:
\begin{align}
    O(\frac{dTN}{\epsilon^2})
\end{align}
gradient calculations, where $N$ is number of random starting points of $\pmb{w}$.

\subsection{Important Technicalities of Gradient Calculation}
\label{subsec:tech-grad}

Classically, gradient at point $\pmb{w}$ is calculated using standard derivative calculation for each dimension:
\begin{align}
    \label{eq:grad1}
    \nabla_{w_i} &= \frac{f(\pmb{w} + \pmb{h_i}) - f(\pmb{w})}{h} \\
    \pmb{h_i} &= \begin{pmatrix}
        0, 0, \hdots, h, \hdots, 0
    \end{pmatrix} & \text{where $h$ is at index $i$}
\end{align}

As we can see the $f(\pmb{w})$ can be reused across all dimensions, and hence we require $f$ to be called $d + 1$ times classically.

Alternatively, we can switch Equation~\ref{eq:grad1} to:
\begin{align}
    \label{eq:grad2}
    \nabla_{w_i} &= \frac{f(\pmb{w} + \frac{\pmb{h_i}}{2}) - f(\pmb{w} - \frac{\pmb{h_i}}{2})}{h}
\end{align}
which the same definition of $\pmb{h_i}$ as above. This requires $f$ to be called $2d$ times, since nothing can be reused.

The Jordan algorithm calculates gradient using the second method, which generally leads to more accurate result.

Additionally for future use, we can rewrite Equation~\ref{eq:grad1}:
\begin{align}
    \label{eq:grad-lemma}
    f(\pmb{w} + \pmb{h_i}) &= f(\pmb{w}) + h\nabla_{w_i}
\end{align}

\section{Jordan's Algorithm}
\label{sec:jord-alg}
\subsection{Prerequisites}
\label{sec:jord-preq}
In this section I will try to provide a much more explicit explanation of needed function oracle,
and its interpretation than presented in the original paper.

Firstly, let us define a classical function $f: x \mapsto y$, where both the domain and the range are fixed point numbers (those can be though of as integers divided by some $2^p$ number).
Let input be $n$-bit precise, and the output to be $2^{n_o}$-bit precise. This means that, for example, if the domain and range are both $[0, 1]$, the distances between distinct inputs and outputs have to be $2^{-n}$ and $2^{-n_o}$, respectively.

For ease of future notation let $2^n = N$ and $2^{n_o} = N_o$.

This allows us to create $f'$ such that $f': x' \mapsto y'$, where $x' \in [0, N]$ and $y' \in [0, N_o]$.
$f'$ on the other hand can be easily converted into a quantum oracle such that:
\begin{align}
    O_f: \ket{x}\ket{a} \mapsto \ket{x}\ket{(a + f'(x)) \mod N_o}
\end{align}

Let us combine this thinking with the gradient calculation from Equation~\ref{eq:grad2}. Let $x \in [-\frac{l}{2}, \frac{l}{2}]$.
Then conversion $f \rightarrow f'$ would split $l$ into $N$ integers, and shift $x$ by $\frac{l}{2}$.

Hence corresponding $f(x) = \frac{l}{N}f'(x' + \frac{N}{2})$, or going the other way:
\begin{equation}
    \label{eq:func-conversion}
    f'(x') = \frac{N}{l}f(x - \frac{l}{2}) = \frac{N}{l}f(\frac{l}{N}(x' - \frac{N}{2}))
\end{equation}

And similarly for the $O_f$ corresponding to the $f'$.

This particular relation between $f, f'$ and $O_f$ is not explicit in~\cite{Jordan_2005},
and only can be understood by transformations between equations, while the transformation from $f$ to $O_f$ is described as trivial.
I believe that because of that this paper lacks a lot of clarity and makes it much harder to reproduce experimentally.

\subsection{Algorithm Explanation}
\label{sec:jord-expl}
Jordan Algorithm is still based on a vanilla gradient descent as mentioned in Section~\ref{subsec:grad-desc},
however gradient calculation itself it done on quantum device as described in~\cite{Jordan_2005}, which is as follows.

An important note is that this gradient is calculated for $\pmb{w} = \pmb{0}$, but we can just shift any function to center it at $0$ perform this calculation, and then shift it back.

\begin{figure}[h]
    \[\Qcircuit @C=1em @R=1em {
        &                 & \gate{H}                    & \qw                        & \multigate{7}{O_f} & \multigate{1}{QFT}         & \qw \\
        &                 & \gate{H}                    & \qw                        & \ghost{O_f}        & \ghost{QFT}                & \qw
        \inputgroupv{1}{2}{0.7em}{1.1em}{\ket{w_1}} \\
        & \lstick{\vdots} & \gate{H^{\otimes (d - 2)n}} & \qw                        & \ghost{O_f}        & \gate{QFT^{\otimes d - 2}} & \qw \\
        &                 & \gate{H}                    & \qw                        & \ghost{O_f}        & \multigate{1}{QFT}         & \qw \\
        &                 & \gate{H}                    & \qw                        & \ghost{O_f}        & \ghost{QFT}                & \qw
        \inputgroupv{4}{5}{0.7em}{1.1em}{\ket{w_{d}}} \\
        &                 & \qw                         & \multigate{2}{QFT^\dagger} & \ghost{O_f}        & \qw                        & \qw \\
        &                 & \qw                         & \ghost{QFT^\dagger}        & \ghost{O_f}        & \qw                        & \qw \\
        &                 & \gate{X}                    & \ghost{QFT^\dagger}        & \ghost{O_f}        & \qw                        & \qw
        \inputgroupv{6}{8}{0.7em}{2em}{\ket{\pmb{out}}} \\
    }\]
    \label{qc:jordans-alg}
    \caption{Circuit Visualization for Jordan's Algorithm}
\end{figure}

Then the above circuit (where inputs are all $\ket{0}$'s) will generate $\ket{\frac{N}{m}\frac{\partial f}{\partial w_1}}, \ket{\frac{N}{m}\frac{\partial f}{\partial w_2}}, \hdots$.

To show this let us consider the transformations of this circuit:
\begin{align}
    \ket{0}^{\otimes nd}\ket{0}^{n_o} &\xrightarrow{H's, X} \\
    \frac{1}{\sqrt{N^d}} \sum\limits_{w} \ket{\pmb{w}} \ket{\pmb{1}} &\xrightarrow{QFT^\dagger} \\
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{w}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{a} &\xrightarrow{O_f} \\
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{w}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{(a + f'(\pmb{w})) \mod 2^N_o} &= \\
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{w}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a + f'(\pmb{w})}{N_o}}\ket{a} &= \\
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{w}e^{i2\pi\frac{f'(\pmb{w})}{N_o}}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{a} &= \\
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{w}e^{i2\pi\frac{N}{ml}f(\frac{l}{N}(w - \frac{N}{2}))}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{a} & & \text{from Equation~\ref{eq:func-conversion}}
\end{align}

Note the appearance of $m$ in the last line. I can be thought of conversion of $N_o$ (max magnitude of $f'$) into magnitude of $f$, and is a maximal magnitude of $\nabla_f$ over the domain given domain.
While I show derivation of relation between $n_o$ and $m$, I will later explicitly use the formula derived in~\cite{Jordan_2005}.

Continuing analysis of the circuit:
\begin{align}
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{w}e^{i2\pi\frac{N}{ml}f(\frac{l}{N}(w - \frac{N}{2}))}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{a} &= \\
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{w}e^{i2\pi\frac{N}{ml}(f(0) + \frac{l}{N}(w - \frac{N}{2})\nabla_f)}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{a} &= & \text{from Equation~\ref{eq:grad-lemma}} \\
    \frac{e^{i2\pi\frac{N}{ml}(f(0) - \frac{N}{2m})}}{\sqrt{N^d N_o}} \sum\limits_{w}e^{i2\pi\frac{w}{m}\nabla_f}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{a} &= \\
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{w}e^{i2\pi\frac{w}{m}\nabla_f}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{a} &\xrightarrow{QFT} & \text{Ignoring Global Phase}\\
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{i = 1}^d \ket{\frac{N}{m}\frac{\partial f}{\partial w_i}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{a} & & \text{Since $\nabla_f$ is just a vector of partial derivatives}
\end{align}

And hence we get a scaled gradient in the input registers.

\section{Numerical Experiment}
\label{sec:num-exp}
\subsection{Problem Description}
For the numerical I decided to calculate gradient of a rather simple function: $f(x) = x^2$.
In this case input is a two dimensional vector, with an arbitrary domain (though I focused on region $[0, 2]$ for numerical reasons).

\subsection{Implementation}
For implementation I have used numpy, because initially I thought it would be the easiest way of implementing the code.
However, now, I would probably use $Q\#$ and study less general problem due to failure of this implementation as explained later in
Section~\ref{subsec:num-results}.

As mentioned before I used:
\begin{itemize}
    \item $d = 2$
    \item $n = 2$
    \item $n_o$ was a hyperparameter I played around with and will discuss its impact in Section~\ref{subsec:num-results}.
    \item $l = \frac{1}{8} = 0.125$
\end{itemize}

While I am aware of the fact that these are not values that are optimal or relate to each other in a consistent way,
as described in Section 5.1 of~\cite{Gily_n_2019}, using them does not significantly change results and conclusions
presented in Section~\ref{subsec:num-results}. However, using them allowed for much faster dev time and testing different
hypotheses about bugs.

Given a binary vector $x$, number of dimensions of input $d$, number of qubits for each input dimension $n$,
number of qubits in the output register $n_o$ and some $d$-dimensional point $p$ for which to calculate $f$,
the $O_f$ was implemented as follows:

\begin{algorithm}[H]
    \SetAlgoLined
    \KwResult{Result of $f$ at point $p$ encoded into $nd + n_o$ dimensional binary vector}
    Let result = 0\;
    \For{\text{i = 0 to d}}{
        Let $x_i$ be the integer with binary representation x[ni:n(i + 1)]\;
        Let $x_i' = l\frac{x_i - \frac{N}{2}}{N} + p_i$\;
        result = result + $x_i'^2$\;
    }

    result = $\lfloor\frac{N}{l}$result$\rfloor$\;

    Let $r$ be result encoded in $n_o$-binary vector ($\mod 2^{n_o}$ if needed)\;
    Add $r$ to the output register of x ($\mod 2^{n_o}$ if needed)\;

    \caption{My Implementation of Jordan's Algorithm}
    \label{alg:my-jordan}
\end{algorithm}

In more intuitive description Algorithm~\ref{alg:my-jordan} performs following steps:
\begin{itemize}
    \item Initialize sum to 0
    \item For each feature:
    \begin{itemize}
        \item Convert feature from $x'$ format to $x$ (to match Equation~\ref{eq:func-conversion})
        \item Add result of $f(x)$ to sum
    \end{itemize}
    \item Convert sum back to $x'$ format (i.e. Integer)
    \item Add to output register
\end{itemize}

I had to also implement QFT, but since the circuit for that given in the class, I will not go into detail.

All of the code (along with TeX and PDF version of this paper) is available in
\href{https://github.com/balbok0/599q1-sp20/tree/master/final_project}{this repository}.

\subsection{Results}
\label{subsec:num-results}

The first and initial test I have performed was to calculate the gradient at $p = \begin{pmatrix} 0, 0 \end{pmatrix}$,
since this exactly the case~\cite{Jordan_2005} described.
For varying $n_o$ I get following results:

\begin{table}[h]
    \centering
    \begin{tabular}{|c||c c c c c|}
        \hline
        $n_o$ & 3 & 4 & 5 & 6 & 7 \\
        \hline
        $P(0)$ & $1 - 9 \cdot 10^{19}$ & $1 - 4 \cdot 10^{19}$ & $1 - 3 \cdot 10^{19}$ & $> 1 - 1 \cdot 10^{19}$ & $> 1 - 1 \cdot 10^{19}$ \\
        \hline
    \end{tabular}
\end{table}

We can see that the algorithm outputs the correct answer (i.e. gradient is equal to $0$) with extremely high probability.
This comes with no surprise since, in this case $O_f = \identity$, and hence for input algorithms the whole circuit evaluates to
$(\text{QFT})(\text{QFT}^\dagger) = \identity$.

However, the implementation of the Jordan's Algorithm starts breaking when we move point $p$ away from the function minima.
Firstly for each point we will need to calculate $m$, which is the maximal derivative
(i.e. $\max \{|\frac{\partial f(p + l)}{\partial x}|, |\frac{\partial f(p - l)}{\partial x}|\} =
\max \{|2(p + l)|, |2(p - l)|\}$).

\begin{table}[h]
    \centering
    \begin{tabular}{|c||c c c|}
        \hline
        $p$ & 0.5 & 1 & 2 \\
        \hline
        $m$ & 1.25 & 2.25 & 4.25 \\
        \hline
        $\frac{N}{m}$ & 3.2 & 1.78 & 0.94 \\
        \hline
        $\frac{N}{m}\frac{\partial f}{\partial x}$ & 3.2 & 3.55 & 3.76 \\
        \hline
    \end{tabular}
    \caption{
        Approximate indexes of states that should have high probabilities after running Jordan's Algorithm for each $p$.
        A point corresponding to value $p$ in this table is $(p, p)$
        (i.e. value is encoded in both dimensions).
    }
    \label{tab:N-m-calc}
\end{table}

We can see that $\frac{N}{m}$ is no longer $0$ so we cannot expect $\ket{\frac{N}{m}\frac{\partial f}{\partial x}}$ to be exact integer.
However, simultaneously we can see that we would expect the highest probabilities in $\ket{3}, \ket{4}$ and $\ket{4}$ respectively.

This is not reflected in the numerical results however, since the respective runs results in:

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c||c c c c c|}
        \hline
        $p$ & $n_o$ & 3 & 4 & 5 & 6 & 7 \\
        \hline
        $0.5$ & $P(3)$ & $0.031$ & $0.037$ & $0.15$ & $0.0043$ & $0.0011$ \\
        \hline
        $1$ & $P(4)$ & $0.0$ & $0.18$ & $0.083$ & $0.022$ & $0.0052$ \\
        \hline
        $2$ & $P(4)$ & $0.0$ & $0.0$ & $0.18$ & $0.083$ & $0.022$ \\
        \hline
    \end{tabular}
    \caption{
        Failed runs of the program.
        The moment we move away from the origin, the algorithm does no longer calculate
        the correct gradient.
    }
    \label{tab:failed-runs-1}
\end{table}

We can see that the probabilities for correct answers are not close to the 1 at all. In fact all of them are below $0.2$.
This means that the algorithm is not implemented correctly, but there is still some debugging,
and analysis to pin-point where the problem is in the implementation.

\subsubsection{Possible Reason for Failure}

Let us look at probabilities of $\ket{0}$ for the failed runs, which are shown in Table~\ref{tab:failed-runs-1}:

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c||c c c c c|}
        \hline
        $p$ & $n_o$ & 3 & 4 & 5 & 6 & 7 \\
        \hline
        $0.5$ & $P(0)$ & $0.18$ & $0.67$ & $0.91$ & $0.98$ & $0.99$ \\
        \hline
        $1$ & $P(0)$ & $0.0$ & $0.18$ & $0.67$ & $0.91$ & $0.98$ \\
        \hline
        $2$ & $P(0)$ & $0.0$ & $0.0$ & $0.18$ & $0.67$ & $0.91$ \\
        \hline
    \end{tabular}
    \caption{
        Probabilities of $P(0)$ state for the failed runs of the algorithm.
    }
    \label{tab:failed-runs-2}
\end{table}

The interesting trend in Table~\ref{tab:failed-runs-2} is a \textit{drift} of $P(0)$ as we increase $n_o$,
or number of qubits in the output register.

This suggests that there might be a significant problem with scaling that happens somewhere along in the algorithm.
It most probably is due to my misunderstanding of some specific part of the algorithm,
since I have tested both QFT and $f(x) = x^2$ for variety of inputs, and they have been providing expected outputs.
I have not been able to exactly pinpoint what is causing the issue but to my best understanding it is within a function:
\begin{lstlisting}[language=Python]
    result = 0
    relative_end = end - start
    half_size = 2**(size_of_dim - 1) * 1.0
    for disp_idx, idx in enumerate(range(0, relative_end - size_of_out, size_of_dim)):
        x = bitstring_to_num(inp[idx:idx + size_of_dim])
        x_arg = (x - half_size) / (2 * half_size) * l - displacements[disp_idx]
        result += x_arg ** 2

    result *= half_size * 2 / l

    clean_result = num_to_bitstring(int(result), size_of_out)
    inp[relative_end - size_of_out:relative_end] = \
        add_bit_strings(inp[relative_end - size_of_out:relative_end], clean_result)
\end{lstlisting}
which tries to implement the Algorithm~\ref{alg:my-jordan}.

This brings me to definition of $m$ which is very vague in~\cite{Jordan_2005}, with its description being
"$m$ is the size of the interval which bound the components of $\nabla f$".
My description of it above Table~\ref{tab:N-m-calc} matches it, but it is very possible that I have not implemented it properly.

Here, I just wanted to conclude my efforts on the practical side of the project. My goal was to work alone trying to reproduce
results from~\cite{Jordan_2005}, and expand on them by comparison to modern gradient calculation tools
(\href{https://github.com/HIPS/autograd}{autograd}, \href{https://github.com/google/jax}{JAX} etc.).
The main motivation behind that was that gradient calculation technique on classical computing also improved significantly over last decade.

However, I believe that what I have shown is that~\cite{Jordan_2005}, while being of huge importance in for Quantum Gradient Calculation,
is not explicit enough to be easily reproducible, and requires testing a lot of assumptions implicit within the text.

\section{Improvements to Jordan's Algorithm}
\label{sec:jor-improv}
\subsection{Different Way of Expressing Jordan's Algorithm}
\cite{Gily_n_2019} presents a modified version of Jordan's Algorithm which uses phase-oracle rather than a bit-oracle.
This allows for a more-intuitive representation of range by just real number. The precision is still bounded by domain,
which is a bit-encoding of a state.

This causes algorithm to use $O(\sqrt{d})$ accesses to a new $0_f$ which is worse than original algorithm (at least for a single estimate),
but still better than a classical calculation.

\subsection{Amplitude Amplification}
\subsection{Grover Search}
\subsection{Additional Improvements}
\textbf{\textcolor{red}{Fix section title}}

\section{Devil's Advocate: Improvements to Classical Gradient Calculation}
\label{sec:class-imp}
\subsection{Active Sampling vs. Passive Sampling}

\section{Conclusion}
\label{sec:conclusion}
% \newpage
\printbibliography
\end{document}
