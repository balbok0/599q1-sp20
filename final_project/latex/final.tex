\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsfonts}
\usepackage{braket}
\usepackage{graphicx}
% \usepackage{thmbox}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage[dvipsnames]{xcolor}
\usepackage{bm}
\usepackage{indentfirst}
\usepackage{qcircuit}

\usepackage[colorlinks,allcolors=blue]{hyperref} % optional
\usepackage[noabbrev,capitalize,nameinlink]{cleveref}

\usepackage[ruled,vlined]{algorithm2e}

\usepackage[style=numeric]{biblatex}
\addbibresource{references.bib}

\usepackage{geometry}
\geometry{margin=1in}

\title{Quantum Gradient Descent via Jordan's Method}
\author{Jakub Filipek}
\date{June 2020}



\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\identity}{\mathds{1}}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{claim}{Claim}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{fact}{Fact}
\newtheorem{corr}{Corollary}[section]
\newtheorem{define}{Definition}
\newtheorem{problem}{Problem}



\begin{document}

\maketitle

\begin{abstract}
    \noindent Gradient Descent has been a key technique for optimizing many Machine Learning Algorithms.
    However, due to extremely large number of parameters of modern models (up to $10^{11}$ floating points)
    this problem has not been widely discussed in Quantum Computing Community.
    In this project, I will try to implement algorithm presented in~\cite{Jordan_2005}.
    Additionally I will follow by discussing potential improvements of this paper, as discussed in~\cite{Gily_n_2019}.
    Both of these are done on example of $f(x) = x^2$.
\end{abstract}

% \section{Introduction}

\section{Introduction to Gradient Descent}
\label{sec:intro-grad-desc}

\subsection{Gradient Descent}
\label{subsec:grad-desc}
Gradient Descent is trying to solve a problem of minimizing (or maximizing) a function:
\begin{align}
    \arg \min \limits_x f(x) | x \in X
\end{align}, where $X$ is a continuos space of numbers (typically in $\mathcal{R}^d$).

Since often finding a global minimum is computationally impossible, a lot of problems reduce to finding local minimum from a given starting point,
and then repeating an experiment for set number of random starting points. In particular, vanilla gradient descent algorithm looks as follows:

\begin{algorithm}[H]
    \SetAlgoLined
    \KwResult{An approximation of local minima of $f$ for starting point $\pmb{w}$, and dataset $X$}
    Let $f$ be parametrized by $\pmb{w}$. \;
    Let $\pmb{w}_0 = \pmb{w}$ \;
    \For{i = 0 to T}{
        Let $\nabla_{\pmb{w}} f(X)$ be an average of gradients of $f$ over dataset X, all with respect to $\pmb{w}$ \;
        $\pmb{w}_{i + 1} = \pmb{w}_i - \alpha \nabla_{\pmb{w}} f(X)$\;
    }
    \Return{$\pmb{w}_T$}
    \caption{Gradient Descent Algorithm}
    \label{alg:vanilla-grad-desc}
\end{algorithm}

While this algorithm does not necessarily lead to the local minimum it performs exceptionally well in practice. It also is a basis for a family of
\textit{gradient descent} algorithms which have been a backbone of last decade's improvements in Machine Learning.

More importantly however, by the above algorithm we can see that we need to calculate the above gradient $O(Td)$ times for each initialization point.
Additionally if we want to achieve $\epsilon$-precise result we need to have $|X| \in O(\frac{1}{\epsilon^2})$, if sampled randomly.

Hence overall, this naive algorithm will take:
\begin{align}
    O(\frac{dTN}{\epsilon^2})
\end{align}
gradient calculations, where $N$ is number of random starting points of $\pmb{w}$.

\subsection{Important Technicalities of Gradient Calculation}
\label{subsec:tech-grad}

Classically, gradient at point $\pmb{w}$ is calculated using standard derivative calculation for each dimension:
\begin{align}
    \label{eq:grad1}
    \nabla_{w_i} &= \frac{f(\pmb{w} + \pmb{h_i}) - f(\pmb{w})}{h} \\
    \pmb{h_i} &= \begin{pmatrix}
        0, 0, \hdots, h, \hdots, 0
    \end{pmatrix} & \text{where $h$ is at index $i$}
\end{align}

As we can see the $f(\pmb{w})$ can be reused across all dimensions, and hence we require $f$ to be called $d + 1$ times classically.

Alternatively, we can switch Equation~\ref{eq:grad1} to:
\begin{align}
    \label{eq:grad2}
    \nabla_{w_i} &= \frac{f(\pmb{w} + \frac{\pmb{h_i}}{2}) - f(\pmb{w} - \frac{\pmb{h_i}}{2})}{h}
\end{align}
which the same definition of $\pmb{h_i}$ as above. This requires $f$ to be called $2d$ times, since nothing can be reused.

The Jordan algorithm calculates gradient using the first method, but has a few inspirations from second method.

Additionally for future use, we can rewrite Equation~\ref{eq:grad1}:
\begin{align}
    \label{eq:grad-lemma}
    f(\pmb{w} + \pmb{h_i}) &= f(\pmb{w}) + h\nabla_{w_i}
\end{align}

\section{Jordan's Algorithm}
\label{sec:jord-alg}
\subsection{Prerequisites}
\label{sec:jord-preq}
In this section I will try to provide a much more explicit explanation of needed function oracle,
and its interpretation than presented in the original paper.

Firstly, let us define a classical function $f: x \mapsto y$, where both the domain and the range are fixed point numbers (those can be though of as integers divided by some $2^p$ number).
Let input be $n$-bit precise, and the output to be $2^{n_o}$-bit precise. This means that, for example, if the domain and range are both $[0, 1]$, the distances between distinct inputs and outputs have to be $2^{-n}$ and $2^{-n_o}$, respectively.

For ease of future notation let $2^n = N$ and $2^{n_o} = N_o$.

This allows us to create $f'$ such that $f': x' \mapsto y'$, where $x' \in [0, N]$ and $y' \in [0, N_o]$.
$f'$ on the other hand can be easily converted into a quantum oracle such that:
\begin{align}
    O_f: \ket{x}\ket{a} \mapsto \ket{x}\ket{(a + f'(x)) \mod N_o}
\end{align}

Let us combine this thinking with the gradient calculation from Equation~\ref{eq:grad2}. Let $x \in [-\frac{l}{2}, \frac{l}{2}]$.
Then conversion $f \rightarrow f'$ would split $l$ into $N$ integers, and shift $x$ by $\frac{l}{2}$.

Hence corresponding $f(x) = \frac{l}{N}f'(x' + \frac{N}{2})$, or going the other way:
\begin{equation}
    \label{eq:func-conversion}
    f'(x') = \frac{N}{l}f(x - \frac{l}{2}) = \frac{N}{l}f(\frac{l}{N}(x' - \frac{N}{2}))
\end{equation}

And similarly for the $O_f$ corresponding to the $f'$.

This particular relation between $f, f'$ and $O_f$ is not explicit in~\cite{Jordan_2005},
and only can be understood by transformations between equations, while the transformation from $f$ to $O_f$ is described as trivial.
I believe that because of that this paper lacks a lot of clarity and makes it much harder to reproduce experimentally.

\subsection{Algorithm Explanation}
\label{sec:jord-expl}
Jordan Algorithm is still based on a vanilla gradient descent as mentioned in Section~\ref{subsec:grad-desc},
however gradient calculation itself it done on quantum device as described in~\cite{Jordan_2005}, which is as follows.

An important note is that this gradient is calculated for $\pmb{w} = \pmb{0}$, but we can just shift any function to center it at $0$ perform this calculation, and then shift it back.

\begin{figure}[h]
    \[\Qcircuit @C=1em @R=1em {
        &                 & \gate{H}                    & \qw                        & \multigate{7}{O_f} & \multigate{1}{QFT}         & \qw \\
        &                 & \gate{H}                    & \qw                        & \ghost{O_f}        & \ghost{QFT}                & \qw
        \inputgroupv{1}{2}{0.7em}{1.1em}{\ket{w_1}} \\
        & \lstick{\vdots} & \gate{H^{\otimes (d - 2)n}} & \qw                        & \ghost{O_f}        & \gate{QFT^{\otimes d - 2}} & \qw \\
        &                 & \gate{H}                    & \qw                        & \ghost{O_f}        & \multigate{1}{QFT}         & \qw \\
        &                 & \gate{H}                    & \qw                        & \ghost{O_f}        & \ghost{QFT}                & \qw
        \inputgroupv{4}{5}{0.7em}{1.1em}{\ket{w_{d}}} \\
        &                 & \qw                         & \multigate{2}{QFT^\dagger} & \ghost{O_f}        & \qw                        & \qw \\
        &                 & \qw                         & \ghost{QFT^\dagger}        & \ghost{O_f}        & \qw                        & \qw \\
        &                 & \gate{X}                    & \ghost{QFT^\dagger}        & \ghost{O_f}        & \qw                        & \qw
        \inputgroupv{6}{8}{0.7em}{2em}{\ket{\pmb{out}}} \\
    }\]
    \label{qc:jordans-alg}
    \caption{Circuit Visualization for Jordan's Algorithm}
\end{figure}

Then the above circuit (where inputs are all $\ket{0}$'s) will generate $\ket{\frac{N}{m}\frac{\partial f}{\partial w_1}}, \ket{\frac{N}{m}\frac{\partial f}{\partial w_2}}, \hdots$.

To show this let us consider the transformations of this circuit:
\begin{align}
    \ket{0}^{\otimes nd}\ket{0}^{n_o} &\xrightarrow{H's, X} \\
    \frac{1}{\sqrt{N^d}} \sum\limits_{w} \ket{\pmb{w}} \ket{\pmb{1}} &\xrightarrow{QFT^\dagger} \\
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{w}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{a} &\xrightarrow{O_f} \\
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{w}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{(a + f'(\pmb{w})) \mod 2^N_o} &= \\
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{w}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a + f'(\pmb{w})}{N_o}}\ket{a} &= \\
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{w}e^{i2\pi\frac{f'(\pmb{w})}{N_o}}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{a} &= \\
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{w}e^{i2\pi\frac{N}{ml}f(\frac{l}{N}(w - \frac{N}{2}))}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{a} & & \text{from Equation~\ref{eq:func-conversion}}
\end{align}

Note the appearance of $m$ in the last line. I can be thought of conversion of $N_o$ (max magnitude of $f'$) into magnitude of $f$, and is a maximal magnitude of $\nabla_f$ over the domain given domain.
While I show derivation of relation between $n_o$ and $m$, I will later explicitly use the formula derived in~\cite{Jordan_2005}.

Continuing analysis of the circuit:
\begin{align}
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{w}e^{i2\pi\frac{N}{ml}f(\frac{l}{N}(w - \frac{N}{2}))}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{a} &= \\
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{w}e^{i2\pi\frac{N}{ml}(f(0) + \frac{l}{N}(w - \frac{N}{2})\nabla_f)}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{a} &= & \text{from Equation~\ref{eq:grad-lemma}} \\
    \frac{e^{i2\pi\frac{N}{ml}(f(0) - \frac{N}{2m})}}{\sqrt{N^d N_o}} \sum\limits_{w}e^{i2\pi\frac{w}{m}\nabla_f}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{a} &= \\
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{w}e^{i2\pi\frac{w}{m}\nabla_f}\ket{\pmb{w}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{a} &\xrightarrow{QFT} & \text{Ignoring Global Phase}\\
    \frac{1}{\sqrt{N^d N_o}} \sum\limits_{i = 1}^d \ket{\frac{N}{m}\frac{\partial f}{\partial w_i}}\sum\limits_{a}e^{i2\pi\frac{a}{N_o}}\ket{a} & & \text{Since $\nabla_f$ is just a vector of partial derivatives}
\end{align}

And hence we get a scaled gradient in the input registers.

\section{Numerical Experiment}
\subsection{Problem Description}
For the numerical I decided to calculate gradient of a rather simple function: $f(x) = x^2$.
In this case input is a two dimensional vector, with an arbitrary domain (though I focused on region $[0, 2]$ for numerical reasons).

\subsection{Implementation}
For implementation I have used numpy, because initially I thought it would be the easiest way of implementing the code.
However, now, I would probably use $Q\#$ and study less general problem due to failure of this implementation as explained later in
Section~\ref{sec:num-results}.

As mentioned before I used:
\begin{itemize}
    \item $d = 2$
    \item $n = 2$
    \item $n_o$ was a hyperparameter I played around with and will discuss its impact in Section~\ref{sec:num-results}.
    \item $l = \frac{1}{8} = 0.125$
\end{itemize}

Given a binary vector $x$, number of dimensions of input $d$, number of qubits for each input dimension $n$,
number of qubits in the output register $n_o$ and some $d$-dimensional point $p$ for which to calulate $f$,
the $O_f$ was implemented as follows:

\begin{algorithm}[H]
    \SetAlgoLined
    \KwResult{Result of $f$ at point $p$ encoded into $nd + n_o$ dimensional binary vector}
    Let result = 0\;
    \For{\text{i = 0 to d}}{
        Let $x_i$ be the integer with binary representation x[ni:n(i + 1)]\;
        Let $x_i' = l\frac{x_i - \frac{N}{2}}{N} + p_i$\;
        result = result + $x_i'^2$\;
    }

    result = $\lfloor\frac{N}{l}$result$\rfloor$\;

    Let $r$ be result encoded in $n_o$-binary vector ($\mod 2^{n_o}$ if needed)\;
    Add $r$ to the output register of x ($\mod 2^{n_o}$ if needed)\;

    \caption{My Implementation of Jordan's Algorithm}
    \label{alg:my-jordan}
\end{algorithm}

In more intuitive description Algorithm~\ref{alg:my-jordan} performs following steps:
\begin{itemize}
    \item Initialize sum to 0
    \item For each feature:
    \begin{itemize}
        \item Convert feature from $x'$ format to $x$ (to match Equation~\ref{eq:func-conversion})
        \item Add result of $f(x)$ to sum
    \end{itemize}
    \item Convert sum back to $x'$ format (i.e. Integer)
    \item Add to output register
\end{itemize}

I had to also implement QFT, but since the circuit for that given in the class, I will not go into detail.

All of the code (along with TeX and PDF version of this paper) is available in
\href{https://github.com/balbok0/599q1-sp20/tree/master/final_project}{this repository}.

\subsection{Results}

The first and initial test I have performed was to calculate the gradient at $p = \begin{pmatrix} 0, 0 \end{pmatrix}$,
since this exactly the case~\cite{Jordan_2005} described.
For varying $n_o$ I get following results:

\begin{table}[h]
    \centering
    \begin{tabular}{|c||c c c c c|}
        \hline
        $n_o$ & 3 & 4 & 5 & 6 & 7 \\
        \hline
        $P(0)$ & $1 - 9 \cdot 10^{19}$ & $1 - 4 \cdot 10^{19}$ & $1 - 3 \cdot 10^{19}$ & $> 1 - 1 \cdot 10^{19}$ & $> 1 - 1 \cdot 10^{19}$ \\
        \hline
    \end{tabular}
\end{table}

We can see that the algorithm outputs the correct answer (i.e. gradient is equal to $0$) with extremely high probability.
This comes with no surprise since, in this case $O_f = \identity$, and hence for input algorithms the whole circuit evaluates to
$(\text{QFT})(\text{QFT}^\dagger) = \identity$.

However, the implementation of the Jordan's Algorithm starts breaking when we move point $p$ away from the function minima.
Firstly for each point we will need to calculate $m$, which is the maximal derivative
(i.e. $\max \{|\frac{\partial f(p + l)}{\partial x}|, |\frac{\partial f(p - l)}{\partial x}|\} =
\max \{|2(p + l)|, |2(p - l)|\}$).

\begin{table}[h]
    \centering
    \begin{tabular}{|c||c c c|}
        \hline
        $p$ & 0.5 & 1 & 2 \\
        \hline
        $m$ & 1.25 & 2.25 & 4.25 \\
        \hline
        $\frac{N}{m}$ & 3.2 & 1.78 & 0.94 \\
        \hline
        $\frac{N}{m}\frac{\partial f}{\partial x}$ & 3.2 & 3.55 & 3.76 \\
        \hline
    \end{tabular}
\end{table}

We can see that $\frac{N}{m}$ is no longer $0$ so we cannot expect $\ket{\frac{N}{m}\frac{\partial f}{\partial x}}$ to be exact integer.
However, simultaneously we can see that we would expect the highest probabilities in $\ket{3}, \ket{4}$ and $\ket{4}$ respectively.

This is not reflected in the numerical results however, since the respective runs results in:
\begin{table}[h]
    \centering
    \begin{tabular}{|c|c||c c c c c|}
        \hline
        $p$ & $n_o$ & 3 & 4 & 5 & 6 & 7 \\
        \hline
        $0.5$ & $P(3)$ & $0.031$ & $0.037$ & $0.15$ & $0.0043$ & $0.0011$ \\
        \hline
        $1$ & $P(4)$ & $0.0$ & $0.18$ & $0.083$ & $0.022$ & $0.0052$ \\
        \hline
        $2$ & $P(4)$ & $0.0$ & $0.0$ & $0.18$ & $0.083$ & $0.022$ \\
        \hline
    \end{tabular}
\end{table}
\subsubsection{Possible Reason for Failure}

\label{sec:num-results}

\section{Improvements to Jordan's Algorithm}
\subsection{Different Way of Expressing Jordan's Algorithm}
\subsection{Amplitude Amplification}
\subsection{Grover Search}
\subsection{Additional Improvements}
\textbf{\textcolor{red}{Fix section title}}

\section{Devil's Advocate: Improvements to Classical Gradient Calculation}
\subsection{Active Sampling vs. Passive Sampling}

\section{Conclusion}
% \newpage
\printbibliography
\end{document}
